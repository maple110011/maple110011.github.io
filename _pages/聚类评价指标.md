---
layout: default.html

---

# 前言

聚类结果的评价无非是从群内的聚集程度和群间的分隔程度出发，再针对某些特殊的数据结构添加一点小小的修正，如果读者学习过“多元统计”这门课程，那么大概会了解到R^2、伪F、半偏R^2和伪t^2等等统计量，但目前中文互联网上似乎尚且没有系统整理出聚类结果评价指标的文章，本文拟对此做一定补全，以供参考

为了内容的简便明了，下面首先给出后文中将要用到的符号一览：

n-样本量

p-变量个数

q-聚类数

$X=x_{ij},i=1,2,\dots,n,j=1,2,\dots,p$，即$n\times p$维的设计阵

$\bar{X}$-$q\times p$维的矩阵，表示类内各变量的均值

$\bar{x}$-X的均值阵

$n_k$-类别$k$中的样本量

$c_k$-类别$k$的均值阵

$x_i$-类别$k$中第$i$个样本，是一个$p$维向量

$\Vert x \Vert=(x^\mathsf{T}x)^{1/2}$，向量的2范数

$W_q =\sum_{k=1}^q\sum_{i\in C_k}\left(x_i-c_k\right)\left(x_i-c_k\right)^\top$，即类内协方差

$B_{q} = \sum_{k=1}^{q}n_{k}\left(c_{k}-\overline{x}\right)\left(c_{k}-\overline{x}\right)^{\top}$，即类间协方差

$N_t=\frac{n(n-1)}{2}$，样本量两两配对的配对数

$N_w=\sum^q_{k=1}\frac{n_k(n_k-1)}{2}$，类内配对数

$N_b=N_t-N_w$，类间配对数

$S_w=\sum_{k=1}^q\sum_{i,j\in C_k}d(x_i,x_j)$，类内距离总和

$S_b=\sum_{k=1}^{q-1}\sum_{l=k+1}^q\sum_{i\in C_k\atop j\in C_l}d(x_i,x_j)$，类间距离总和

# 聚类评价指标

## CH统计量

由Calinski和Harabasz提出的指标[参考]，其定义为
$$CH(q)=\frac{tr(B_q)/(q-1)}{tr(W_q)/(n-q)}$$
[解释]，故要选取聚类数$q$使得CH统计量最大化

## Duda统计量
由Duda和Hart提出的[参考]，其定义为
$$Duda=\frac{Je(2)}{Je(1)}=\frac{W_k+W_l}{W_m}$$
其中$Je(2)$是将样本仅分为两类时的均方误差，$Je(1)$是不分类时的均方误差
Gordon[参考]经研究发现，应当选取最小的聚类数使得
$$Duda\geq 1-\frac{2}{\pi q}-z\sqrt{\frac{2(1-\frac{8}{\pi^2q})}{n_mq}}=critValueDuda$$
注：q和p是否混淆？
其中$z$是某一参数，经Milligan和Cooper[参考]的模拟实验，他们建议选取$z=3.20$

## 伪t^2统计量

由Duda和Hart提出的[参考]，其定义为
$$t^2=\frac{V_{kl}}{\frac{W_k+W_l}{n_k+n_l-2}}$$
其中$V_{kl}=W_m-W_k-W_l,C_m=C_k\cup C_l$
需要指出，伪t^2统计量仅适用于层次聚类法

Gordon[参考]指出应当选取满足下式的最小的$q$
$$t^2\leq\left(\frac{1-critValueDuda}{critValueDuda}\right)\times(n_k+n_l-2)$$

## C统计量

由Hubert和Levin提出的，其定义如下
$$\mathrm{Cindex}=\frac{S_w-S_{\min}}{S_{\max}-S_{\min}}, S_{\min}\neq S_{\max}, \mathrm{Cindex}\in(0,1)$$
其中$S_{\min}$是全体配对样本中$N_w$个最小的距离的总和，$S_{\max}$是全体配对样本中$N_w$个最大的距离的总和
Milligan和Cooper[参考]，Gordon[参考]的研究指出，C统计量的最小值可以作为最优聚类数

## Gamma统计量

Gamma统计量其实是由Gamma相关系数推广得到的，Gamma相关系数也称为Goodman and Kruskal统计量，衡量的是定序数据中的相关系数，[解释]

其定义为
$$Gamma=\frac{s(+)-s(-)}{s(+)-s(-)}$$
其中$s(+)$是协和对数量，$s(-)$是非协和对数量
Milligan和Cooper[参考]的研究表明，Gamma统计量的最大值可以作为最优聚类数

## Beale统计量

由Beale[参考]提出的，构造了一个F统计量来检验如下问题

$H_0$:存在$q_1$个聚类 vs $H_1$:存在$q_2$个聚类

其中$q_2>q_1$

检验统计量的形式为
$$\mathrm{Beale}=F\equiv\frac{\left(\frac{V_{kl}}{W_k+W_l}\right)}{\left(\frac{n_m-1}{n_m-2}\right)2^{\frac{2}{p}}-1}$$
其中$V_{kl}=W_m-W_k-W_l,C_m=C_k\cup C_l$，通过比较F统计量与临界值$F_{(p,n_m-2)p}$的关系来决定最优聚类数

Gordon的研究[参考]指出当$q_1=1$时，拒绝原假设通常需要一个非常大的F值

注：F统计量临界值需研究

## CCC统计量

全称为Cubic Clustering Criterion，由Sarle提出的一个统计量，其定义为
$$\mathrm{CCC}=\ln\left[\frac{1-\mathrm{E}\left(R^2\right)}{1-R^2}\right]\frac{\sqrt{\frac{np^*}{2}}}{\left(0.001+\mathrm{E}\left(R^2\right)\right)^{1.2}}$$
其中
$$R^2=1-\frac{\operatorname{trace}(X^\top X-\overline{X}^\top Z^\top Z\overline{X})}{\operatorname{trace}(X^\top X)}$$
Z是$n\times q$维的类别指示阵，其元素$z_{ik}$表示样本$i$是否属于类别$k$，$\overline{X}=(Z^\top Z)^{-1}Z^\top X$

注：该统计量较复杂，需要深入研究

### Ptbiserial统计量

该统计量由Milligan和Kraemer[参考]提出，是一种点二列相关系数(point-biserial correlation)，
其公式如下
$$\text{Ptbiserial}=\frac{\begin{bmatrix}\overline{S}_b-\overline{S}_w\end{bmatrix}\begin{bmatrix}N_wN_b/N_t^2\end{bmatrix}^{1/2}}{s_d}$$
其中$\overline{S}_w=\frac{S_w}{N_w}$，$\overline{S}_b=\frac{S_b}{N_b}$，$s_d$为所有距离的标准差

Milligan和Cooper[参考]的研究指出应当选择使Ptbiserial统计量最大化的聚类数

### Gplus统计量

由Rohlf[参考]提出的，其公式如下
$$\text{Gplus}=\frac{2s(-)}{N_t\left(N_t-1\right)}$$
其中$s(-)$是非协和对的数量，也就是说，它意味着同一类别内的两点之间距离大于不同类别中两点之间距离的次数，故应当选择使Gplus统计量最小化的类别数，当然，这也是Milligan和Cooper[参考]的研究指出的

### DB统计量

由Davies和Bouldin提出，其公式如下
$$\mathrm{DB}(q)=\frac{1}{q}\sum_{k=1}^{q}\max_{k\neq l}\left(\frac{\delta_k+\delta_l}{d_{kl}}\right)$$
其中下标$k,l$指示类别，$d_{kl}$表示类别$C_k$和$C_l$的质心之间的某种距离，即$d_{kl} = \sqrt[v]{\sum_{j=1}^p|c_{kj}-c_{lj}|^v}$，特别地，当$v=2$时$d_{kl}$就是Euclidean距离；$\delta_k$是$C_k$的某种离散情况度量，即$\delta_k = \sqrt[u]{\frac{1}{n_k}\sum\limits_{i\in C_kj=1}^p|x_{ij}-c_{kj}|^u}$，特别地，当$u=2$时，$\delta_k$就是$C_k$中样本的标准差

可以看到，DB统计量实际上就是类内散度和类间分离度的比值(需要注意这里是类间分离度而非类间散度，二者的区别在于)

Milligan和Cooper[参考]；Davies和Bouldin[参考]的研究指出应当选择使得DB统计量最小化的类别数$q$

### Frey统计量

由Frey和Van Groenewoud[参考]提出，其公式如下
$$\text{Frey}=\frac{\overline{S}_{b_{j+1}}-\overline{S}_{b_j}}{\overline{S}_{w_{j+1}}-\overline{S}_{w_j}}$$
其中，$\overline{S}_b=\frac{S_b}{N_b}$为平均类间距离，$\overline{S}_w=\frac{S_w}{N_w}$为平均类内距离，下标$j$表示系统聚类的层次

可以看到，该统计量实际上就是

Milligan和Cooper[参考]的研究指出，应该不断减小聚类数直到Frey统计量跌破1为止，如果它一直没有跌到1以下，那就应该假定只存在一个类别

### Hartigan统计量

由Hartigan[参考]提出，其公式如下
$$\text{Hartigan}=\left(\frac{\text{trace}(W_q)}{\text{trace}(W_{q+1})}-1\right)(n-q-1)$$
其中$q\in\{1,\cdots,n-2\}$，Milligan和Cooper[参考]的研究指出，应当选择使得两个类别数之间的Hartigan统计量

注：需要参考原文，到底怎么选取聚类数

### Tau统计量
由Rohlf提出，并经Milligan测试，其公式如下
$$\text{Tau}=\frac{s(+)-s(-)}{\left[\left(N_t\left(N_t-1\right)/2-t\right)\left(N_t\left(N_t-1\right)/2\right)\right]^{1/2}}$$
其中$s(+)$为协和对数量，即两个同类样本间的距离小于两个不同类样本间距离的次数，$s(-)$为非协和对数量，即两个同类样本间的距离大于两个不同类样本间距离的次数，$N_t$为两两配对的配对数

Milligan和Cooper[参考]的研究指出，应当选择使得Tau统计量最大化的类别数

需要注意的是，由于Tau统计量的计算比较耗时，仅当index参数为tau或alllong时才会计算Tau统计量

### Ratkowsky统计量

由Ratkowsky和Lance[参考]提出，其公式为
$$Ratkowsky=\frac{\overline{S}}{q^{1/2}}$$
其中$\overline{S}$为$BGSS_j/TSS_j$的平均值，BGSS为单个变量的组间平方和，TSS为单个变量的总平方和

注：BGSS和TSS检验更换缩写，$\overline{S}$的计算公式似乎可以简化

### Scott统计量

由Scott和Symons[参考]提出，其公式如下
$$\text{Scott}=n\log\frac{\det(T)}{\det(W_q)}$$
其中$n$为样本量，$T$为总平方和，$W_q$为q个类别下的组内平方和

Milligan和Cooper[参考]的研究指出，应当选择使得

### Marriot统计量

由Marriot提出，其公式如下
$$Marriot=q^2det(W_q)$$
Milligan和Cooper[参考]的研究指出，应当选择使得

